import 'server-only';

export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface LLMConfig {
  provider: 'openai' | 'deepseek' | 'custom';
  apiKey?: string;
  apiUrl?: string;
  model?: string;
  maxTokens?: number;
  temperature?: number;
}

export interface LLMResponse {
  content: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

class LLMClient {
  private config: LLMConfig;

  constructor(config?: Partial<LLMConfig>) {
    // Set initial config with defaults
    this.config = {
      provider: 'deepseek',
      maxTokens: 500,
      temperature: 0.7,
      ...config,
    };
    
    // Override with environment variables if available
    if (process.env.LLM_PROVIDER) {
      this.config.provider = process.env.LLM_PROVIDER as any;
    }
    if (process.env.LLM_API_KEY) {
      this.config.apiKey = process.env.LLM_API_KEY;
    }
    if (process.env.LLM_MAX_TOKENS) {
      this.config.maxTokens = parseInt(process.env.LLM_MAX_TOKENS);
    }
    if (process.env.LLM_TEMPERATURE) {
      this.config.temperature = parseFloat(process.env.LLM_TEMPERATURE);
    }
    
    // Set API URL and model based on provider
    if (!this.config.apiUrl) {
      this.config.apiUrl = process.env.LLM_API_URL || this.getDefaultApiUrl();
    }
    if (!this.config.model) {
      this.config.model = process.env.LLM_MODEL || this.getDefaultModel();
    }
  }

  private getDefaultApiUrl(): string {
    const provider = this.config.provider || 'deepseek';
    switch (provider) {
      case 'openai':
        return 'https://api.openai.com/v1/chat/completions';
      case 'deepseek':
        return 'https://api.deepseek.com/v1/chat/completions';
      default:
        return process.env.LLM_API_URL || 'http://localhost:8080/v1/chat/completions';
    }
  }

  private getDefaultModel(): string {
    const provider = this.config.provider || 'deepseek';
    switch (provider) {
      case 'openai':
        return 'gpt-3.5-turbo';
      case 'deepseek':
        return 'deepseek-chat';
      default:
        return 'default-model';
    }
  }

  async chat(messages: LLMMessage[]): Promise<LLMResponse> {
    // For custom provider with mock mode, return intelligent mock responses
    if (this.config.provider === 'custom' && this.config.apiKey === 'mock-key') {
      return this.generateMockResponse(messages);
    }

    if (!this.config.apiKey && this.config.provider !== 'custom') {
      throw new Error(`API key required for ${this.config.provider} provider`);
    }

    try {
      const headers: HeadersInit = {
        'Content-Type': 'application/json',
      };

      if (this.config.apiKey) {
        headers['Authorization'] = `Bearer ${this.config.apiKey}`;
      }

      const response = await fetch(this.config.apiUrl!, {
        method: 'POST',
        headers,
        body: JSON.stringify({
          model: this.config.model,
          messages,
          max_tokens: this.config.maxTokens,
          temperature: this.config.temperature,
        }),
      });

      if (!response.ok) {
        const error = await response.text();
        throw new Error(`LLM API error: ${response.status} - ${error}`);
      }

      const data = await response.json();

      return {
        content: data.choices[0]?.message?.content || '',
        usage: data.usage ? {
          promptTokens: data.usage.prompt_tokens,
          completionTokens: data.usage.completion_tokens,
          totalTokens: data.usage.total_tokens,
        } : undefined,
      };
    } catch (error) {
      console.error('LLM chat error:', error);
      throw error;
    }
  }

  private generateMockResponse(messages: LLMMessage[]): LLMResponse {
    const lastUserMessage = messages.filter(m => m.role === 'user').pop();
    const userInput = lastUserMessage?.content || '';
    const lowerInput = userInput.toLowerCase();

    // Generate contextual responses based on input
    let response = '';
    
    if (lowerInput.includes('ä½ å¥½') || lowerInput.includes('hello') || lowerInput.includes('hi')) {
      const greetings = [
        'ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½  ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ',
        'å—¨ï¼ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿ',
        'ä½ å¥½å‘€ï¼æˆ‘æ˜¯ä½ çš„AIåŠ©æ‰‹ï¼Œéšæ—¶ä¸ºä½ æœåŠ¡ï¼',
      ];
      response = greetings[Math.floor(Math.random() * greetings.length)];
    } else if (lowerInput.includes('å¤©æ°”')) {
      response = 'ä»Šå¤©å¤©æ°”çœ‹èµ·æ¥ä¸é”™å‘¢ï¼è®°å¾—é€‚æ—¶å¢å‡è¡£ç‰©ï¼Œä¿æŒå¥åº·å“¦ï½';
    } else if (lowerInput.includes('æ€ä¹ˆæ ·') || lowerInput.includes('å¦‚ä½•')) {
      response = 'è¿™æ˜¯ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼è®©æˆ‘æƒ³æƒ³...æˆ‘è§‰å¾—è¿™å–å†³äºå…·ä½“æƒ…å†µã€‚ä½ èƒ½å‘Šè¯‰æˆ‘æ›´å¤šç»†èŠ‚å—ï¼Ÿ';
    } else if (lowerInput.includes('è°¢è°¢')) {
      response = 'ä¸å®¢æ°”ï¼èƒ½å¸®åˆ°ä½ æˆ‘å¾ˆå¼€å¿ƒ ğŸ˜Š';
    } else if (lowerInput.includes('å†è§') || lowerInput.includes('bye')) {
      response = 'å†è§ï¼æœŸå¾…ä¸‹æ¬¡èŠå¤©ï¼Œç¥ä½ æœ‰ç¾å¥½çš„ä¸€å¤©ï¼ğŸ‘‹';
    } else if (lowerInput.includes('åƒ') || lowerInput.includes('é¥­')) {
      response = 'è¯´åˆ°åƒçš„ï¼Œæˆ‘è™½ç„¶ä¸èƒ½å“å°ç¾é£Ÿï¼Œä½†æˆ‘çŸ¥é“ç¾é£Ÿèƒ½å¸¦æ¥å¹¸ç¦æ„Ÿï¼ä½ æœ€å–œæ¬¢ä»€ä¹ˆèœå‘¢ï¼Ÿ';
    } else if (lowerInput.includes('å·¥ä½œ') || lowerInput.includes('å¿™')) {
      response = 'å·¥ä½œè™½ç„¶é‡è¦ï¼Œä½†ä¹Ÿè¦è®°å¾—åŠ³é€¸ç»“åˆå“¦ï¼é€‚å½“çš„ä¼‘æ¯èƒ½è®©ä½ æ›´æœ‰æ•ˆç‡ã€‚';
    } else {
      // Default intelligent responses
      const defaults = [
        'å—¯ï¼Œæˆ‘æ˜ç™½ä½ çš„æ„æ€ã€‚è¿™ç¡®å®æ˜¯ä¸ªå€¼å¾—æ€è€ƒçš„è¯é¢˜ã€‚',
        'æœ‰æ„æ€ï¼ä½ èƒ½è¯¦ç»†è¯´è¯´å—ï¼Ÿæˆ‘å¾ˆæƒ³äº†è§£æ›´å¤šã€‚',
        'æˆ‘è§‰å¾—ä½ è¯´å¾—å¾ˆæœ‰é“ç†ã€‚ä»å¦ä¸€ä¸ªè§’åº¦çœ‹...',
        'è¿™è®©æˆ‘æƒ³èµ·äº†ä¸€å¥è¯ï¼š"ç”Ÿæ´»å°±åƒä¸€ç›’å·§å…‹åŠ›ï¼Œä½ æ°¸è¿œä¸çŸ¥é“ä¸‹ä¸€é¢—æ˜¯ä»€ä¹ˆå‘³é“ã€‚"',
        'ç¡®å®å¦‚æ­¤ï¼æ¯ä¸ªäººéƒ½æœ‰è‡ªå·±ç‹¬ç‰¹çš„è§è§£å’Œä½“éªŒã€‚',
      ];
      response = defaults[Math.floor(Math.random() * defaults.length)];
    }

    return {
      content: response,
      usage: {
        promptTokens: 10,
        completionTokens: 20,
        totalTokens: 30,
      },
    };
  }

  async generateResponse(
    userMessage: string,
    systemPrompt?: string,
    history?: LLMMessage[]
  ): Promise<string> {
    const messages: LLMMessage[] = [];

    if (systemPrompt) {
      messages.push({
        role: 'system',
        content: systemPrompt,
      });
    }

    if (history) {
      messages.push(...history);
    }

    messages.push({
      role: 'user',
      content: userMessage,
    });

    const response = await this.chat(messages);
    return response.content;
  }
}

// Simple response cache to avoid repeated API calls
const responseCache = new Map<string, { response: string; timestamp: number }>();
const CACHE_TTL = 1000 * 60 * 5; // 5 minutes

export function getCachedResponse(key: string): string | null {
  const cached = responseCache.get(key);
  if (cached && Date.now() - cached.timestamp < CACHE_TTL) {
    return cached.response;
  }
  responseCache.delete(key);
  return null;
}

export function setCachedResponse(key: string, response: string): void {
  // Limit cache size
  if (responseCache.size > 100) {
    const firstKey = responseCache.keys().next().value;
    responseCache.delete(firstKey);
  }
  responseCache.set(key, { response, timestamp: Date.now() });
}

export default LLMClient;